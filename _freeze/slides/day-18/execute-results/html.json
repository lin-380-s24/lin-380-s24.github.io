{
  "hash": "b04ae8364b02a67f2b0f64bc623cac97",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Explore\"\nsubtitle: |\n  \"The data speaks for itself, but only if you are willing to listen.\"\\\n  --- Nate Silver\"\ndate: \"2024-03-27\"\ncategories: [analysis, frequency analysis, co-occurrence analysis, clustering, dimensionality reduction, word embeddings]\n---\n\n\n## Overview\n\n- Orientation\n- Descriptive analysis\n- Unsupervised learning\n\n![](images/fig-eda-masc-vsm-word2vec-manner-motion-compare-1.png){.absolute top=320 right=10 width=550}\n\n![](images/fig-eda-masc-verb-part-network-1.png){.absolute top=50 right=50 width=475}\n\n## Process\n\n![The Big Picture](images/qtal-big-picture.png)\n\n![](images/you-are-here.drawio.png){.absolute top=50 right=400}\n\n# Orientation\n\n## Exploratory data analysis\n\n:::: {.columns}\n::: {.column width=\"33%\"}\n**Goals**\n\n- Discover patterns\n- Describe emerging relationships\n- Posit hypotheses\n:::\n\n::: {.column width=\"33%\"}\n**When to use**\n\n- Literature is scarce\n- Gap in knowledge is wide\n- New territory\n:::\n\n::: {.column width=\"33%\"}\n**How to use**\n\n- Identify, Inspect, Interrogate, Interpret\n- Iterative process\n  - Update: unit of observation, variables, methods\n:::\n::::\n\n# Descriptive analysis\n\n## Frequency analysis: what is it?\n\nSimply put, its counting tokens.\n\n| Method | Description | |\n|--------|-------------|--|\n| Raw frequency | Number of occurrences of a token within a corpus | {{< fa arrows-up-down >}} |\n| Dispersion | Distribution of a token across a corpus | {{< fa arrows-left-right >}} |\n| Relative frequency | Proportion of a token in relation to the total number of tokens in a corpus | {{< fa arrow-down-up-across-line >}} |\n\n: {tbl-colwidths=\"[30, 60, 10]\"}\n\n## What can a frequency analysis tell us?\n\n- Varying usage and distributin of tokens can signal:\n  - Different populations of speakers\n  - Different topics, genres, registers, etc.\n  - Cognitive processes (entrenchment, salience)\n\n## Things to consider\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- Tokenization\n- Operationalization\n- Zipf distribution\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {#fig-zipf-distribution .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Original](day-18_files/figure-revealjs/fig-zipf-distribution-1.png){#fig-zipf-distribution-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Log-transformed](day-18_files/figure-revealjs/fig-zipf-distribution-2.png){#fig-zipf-distribution-2 width=480}\n:::\n\nZipf distribution (N = 100)\n:::\n\n:::\n::::\n\n::: {.notes}\n- Define what constitutes a token\n- Balancing frequency and importance\n- Zipf distribution: a small number of tokens are very frequent, while the majority are rare\n- Log-transformed data can help decompress the distribution\n- Zipf's law: the frequency of a token is inversely proportional to its rank\n:::\n\n## Co-occurrence analysis: what is it?\n\nIdentify patterns of association between tokens\n\n| Method | Description | |\n|--------|-------------|--|\n| n-grams | Sequence of n tokens | {{< fa ellipsis >}} |\n| Collocation | Tokens that frequently co-occur | {{< fa street-view >}} |\n\n: {tbl-colwidths=\"[30, 60, 10]\"}\n\n\n## What can a co-occurrence analysis tell us?\n\nPatterns of association between tokens can signal:\n\n- Grammatical structures\n- Semantic relationships\n- Formulaic/ Idomatic expressions\n\n## Things to consider\n\n- Operationalization\n- Context window\n- Measures of association\n  - Pointwise mutual information\n  - Log-likelihood ratio\n\n::: {.notes}\n- Define what type of association you are interested in\n- Context window: the number of tokens before and after the target token\n- Measures trade-offs:\n  - PMI: strong and frequent associations, sensitive to rare tokens\n  - LLR: unexpected and rare associations.\n:::\n\n# Unsupervised learning\n\n## Clustering: what is it?\n\nBottom-up approach to grouping similar data points\n\n| Method | Description | |\n|--------|-------------|--|\n| K-means | Partition data into k clusters | {{< fa circle-nodes >}} |\n| Hierarchical clustering | Build a tree of clusters | {{< fa sitemap >}} |\n\n## What can clustering tell us?\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n- Patterns of similarity/ dissimilarity\n- Number of groups\n- Homogeneity within groups\n- Heterogeneity between groups\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {#fig-kmeans-hclust-examples .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![K-means](day-18_files/figure-revealjs/fig-kmeans-hclust-examples-1.png){#fig-kmeans-hclust-examples-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Hierarchical clustering](day-18_files/figure-revealjs/fig-kmeans-hclust-examples-2.png){#fig-kmeans-hclust-examples-2 width=480}\n:::\n\nExamples of K-means and hierarchical clustering\n:::\n\n:::\n::::\n\n::: {.aside}\nThe `factoextra` package [@R-factoextra] provides functions to visualize clustering results.\n:::\n\n## Things to consider\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- Previous knowledge\n- Number of clusters\n- Distance metric (Heirarchical clustering)\n:::\n\n::: {.column width=\"50%\"}\n![](images/fig-eda-masc-pos-kmeans-elbow-1.png)\n:::\n::::\n\n::: {.aside}\nThe `proxy` package [@R-proxy] provides a `dist()` function which includes the cosine distance metric which is useful for Zipfian distribution cases.\n:::\n\n::: {.notes}\n- Previous knowledge can help determine the number of clusters to compare to\n- The elbow method can help determine the optimal number of clusters\n- Distance metrics:\n  - Cosine similarity: measures the cosine of the angle between two vectors (reduces the effect of magnitude, in Zipfian distribution cases)\n  - Euclidean distance: straight-line distance between two points\n:::\n\n## Dimensionality reduction: what is it?\n\nOperation to reduce the number of variables in a dataset, while preserving as much information as possible\n\n| Method | Description | |\n|--------|-------------|--|\n| PCA | Linear transformation to reduce dimensionality | {{< fa person-walking-arrow-right >}} {{< fa table >}} |\n| t-SNE | Non-linear transformation to visualize high-dimensional data | {{< fa person-walking-arrow-loop-left >}} {{< fa table >}} |\n\n: {tbl-colwidths=\"[20, 60, 20]\"}\n\n## What can dimensionality reduction tell us?\n\n- How variables are related\n- How redundant variables are\n- Identify dimensions that explain the most variance\n\n## Things to consider\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n- Interpretability\n- Number of dimensions\n- Overfitting/ Underfitting\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {#fig-kmeans-with-and-without-pca .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Without PCA](day-18_files/figure-revealjs/fig-kmeans-with-and-without-pca-1.png){#fig-kmeans-with-and-without-pca-1 width=480}\n:::\n\n::: {.cell-output-display}\n![With PCA](day-18_files/figure-revealjs/fig-kmeans-with-and-without-pca-2.png){#fig-kmeans-with-and-without-pca-2 width=480}\n:::\n\nK-means clustering with and without PCA\n:::\n\n:::\n::::\n\n::: {.aside}\n`factoextra` also provides functions to visualize PCA results.\n:::\n\n::: {.notes}\n- Interpretability: is the reduced dimensionality still interpretable?\n- Number of dimensions: how many dimensions are needed to explain the most variance?\n- Overfitting: is the model too complex? Can it generalize to new data? PCA can help reduce overfitting, but may lead to underfitting.\n:::\n\n## Word embeddings: what is it?\n\nUse of distributed representations of words in a continuous vector space where words with similar contextual distributions are closer together\n\n| Method | Description | |\n|--------|-------------|--|\n| Word2Vec | Popular word embedding model | {{< fa street-view >}} {{< fa arrow-right >}} {{< fa layer-group >}} |\n| GloVe | Global vectors for word representation | {{< fa street-view >}} {{< fa arrow-right >}} {{< fa layer-group >}} |\n\n: {tbl-colwidths=\"[20, 60, 20]\"}\n\n[Note: word embedding models are highly contingent on the size of the corpus, the algorithm used, and parameters set.]{style=\"font-size: 0.5em;\"}\n\n::: {.aside}\nThe `word2vec` package [@R-word2vec] provides functions to train and use Word2Vec models.\n:::\n\n## What can word embeddings tell us?\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- Semantic relationships between words\n- Grammatical relationships between words\n- Analogies between words\n- Clustering of words based on context\n:::\n\n::: {.column width=\"50%\"}\n![](images/fig-eda-masc-vsm-word2vec-manner-motion-compare-1.png)\n:::\n::::\n\n## Things to consider\n\n- Context window\n- Number of dimensions\n- Pre-trained models vs. training your own model\n\n::: {.notes}\n- Context window: the number of tokens before and after the target token\n- Number of dimensions: how many dimensions are needed to explain the most variance?\n- Training data: pre-trained models are useful for general tasks, but training your own model can be more specific to your data (size of the corpus, domain-specific vocabulary)\n:::\n\n# Wrap-up\n\n## Final thoughts\n\n- Exploratory data analysis covers a wide range of methods and techniques.\n- Of the three approaches we will cover, it requires the most creativity and flexibility.\n- The evaluative process is iterative and associative, and the results are often open to interpretation.\n\n## References\n",
    "supporting": [
      "day-18_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}