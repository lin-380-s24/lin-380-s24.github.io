{
  "hash": "675a8885b1d1ba9fd3e9084c083782de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predict\"\nsubtitle: |\n  \"All models are wrong, but some are useful.\"\\\n  --- George E.P. Box\ndate: \"2024-04-03\"\ncategories: [analysis, prediction, supervised-learning, machine-learning, text-analysis, tidymodels]\nwebr:\n  show-startup-message: false\n  packages: ['tidymodels', 'fs', 'stringr', 'textrecipes', 'LiblineaR', 'ranger']\n  message: false\n---\n\n\n## Overview\n\n- Orientation\n- Predictive modeling\n  - Workflow with `tidymodels`\n\n\n```{=html}\n<style>\ntable {\n  font-size: 0.8em;\n}\n</style>\n```\n\n\n\n\n```{webr-r}\n#| context: setup\n\ndownload.file(\n  \"https://raw.githubusercontent.com/lin-380-s24/lin-380-s24.github.io/main/slides/data/satisfaction-harmony-data.csv\", \"satisfaction-harmony-data.csv\"\n)\n\ntbl <- read.csv(\"satisfaction-harmony-data.csv\")\n\nsatis_tbl <-\n  tbl |>\n  select(\n    gender,\n    age,\n    text = satisfactiontexts\n  ) |>\n  mutate(\n    gender = factor(gender),\n    text = str_trim(text)\n  )\n```\n\n\n## Process\n\n![The Big Picture](images/qtal-big-picture.png)\n\n![](images/you-are-here.drawio.png){.absolute top=50 right=310}\n\n# Orientation\n\n## Predictive Data Analysis\n\n:::: {.columns}\n::: {.column width=\"33%\"}\n**Goals**\n\n- Prescribe actions\n- Examine outcome-predictor relationship\n- Assess hypotheses\n:::\n\n::: {.column width=\"33%\"}\n**When to use**\n\n- To perform tasks\n- Specific knowledge gap\n- Alternative to inference\n:::\n\n::: {.column width=\"33%\"}\n**How to use**\n\n- Identify, Inspect, Interrogate, Interpret\n- Iterative:\n  - Features, Model\n:::\n::::\n\n# Predictive modeling\n\n## Classification vs. Regression tasks {.smaller}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- **Classification**: Predicting a categorical variable\n:::\n\n::: {.column width=\"50%\"}\n- **Regression**: Predicting a continuous variable\n:::\n::::\n```{webr-r}\n# View dataset\n\nglimpse(satis_tbl)\n```\n\n## Features: tokenization {.smaller .scrollable}\n\nIn text analysis, features are often linguistic units (tokens).\n\n```{webr-r}\n#| context: setup\n\nview_rec <- function(rec, n = 5) {\n  rec |>\n    prep() |>\n    bake(new_data = NULL) |>\n    slice_head(n = n)\n}\n```\n\n```{webr-r}\n# Tokenization\nrecipe(\n    formula = gender ~ text,      #<1> outcome ~ predictors\n    data = satis_tbl) |>\n    step_tokenize(text) |>        #<2> default words\n    step_tokenfilter(text, max_tokens = 5) |>\n    step_tf(text) |>              #<3> term frequency (relative)\n  view_rec()                      #<4> custom function\n```\n\n## Features: metadata {.smaller .scrollable}\n\n But they can also be other types of variables such as metadata.\n\n```{webr-r}\n# Metadata features\nrecipe(\n    formula = gender ~ age + text, #<1> outcome ~ predictors\n    data = satis_tbl) |>\n  view_rec()                       #<2> custom function\n```\n\n## Features: text features {.smaller .scrollable}\n\nOr derived features.\n\n```{webr-r}\n# Text features\n# ?count_functions                #<0> list of possible features\nrecipe(\n    formula = gender ~ text,      #<1> outcome ~ predictors\n    data = satis_tbl) |>\n  step_textfeature(text) |>       #<2> default features\n  view_rec()                      #<3> custom function\n```\n\n## Workflow with `tidymodels` {.smaller}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**A. Identify**\n\n- Variables\n- Splits\n- Recipe\n\n**B. Inspect**\n\n- Features\n:::\n\n::: {.column width=\"50%\"}\n**C. Interrogate**\n\n- Model\n- Tune\n- Fit\n- Evaluate\n\n**D. Interpret**\n\n- Predict\n- Evaluate\n- Explore\n:::\n::::\n\n::: {.aside}\n{tidymodels} [@R-tidymodels] is a collection of packages for modeling and machine learning using the tidyverse.\n:::\n\n## Identify: variables {.smaller .scrollable}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- **Outcome variable**: The variable you want to predict\n- **Predictor variables**: The variables you will use to make the prediction\n:::\n\n::: {.column width=\"50%\"}\n| Variable | Type | Description |\n|----------|------|-------------|\n| `gender` | Outcome | Aim to predict 'female' or 'male' |\n| `text` | Predictor | Text data to predict `gender` |\n:::\n::::\n\n```{webr-r}\n# Prep data\ncls_tbl <-\n  satis_tbl |>\n  select(outcome = gender, text)\n\n# Preview\nglimpse(cls_tbl)\n```\n\n```{webr-r}\n#| context: setup\n# Prep data\ncls_tbl <-\n  satis_tbl |>\n  select(outcome = gender, text)\n```\n\n## Identify: Splits  {.smaller .scrollable}\n\n- **Training set**: Used to train, tune, and evaluate the model\n- **Testing set**: Used to evaluate the final model\n\n```{webr-r}\n# Split data\ncls_split <-\n  initial_split(                     #<1> split data\n    data = cls_tbl,\n    prop = 0.75,                     #<2> proportion\n    strata = outcome                 #<3> stratify by `gender`\n)\ncls_trn <- training(cls_split)       #<4> training set\ncls_tst <- testing(cls_split)        #<5> testing set\n\n# Preview splits\ncls_trn |> count(outcome)\ncls_tst |> count(outcome)\n```\n\n```{webr-r}\n#| context: setup\n# Split data\ncls_split <-\n  initial_split(                     #<1> split data\n    data = cls_tbl,\n    prop = 0.75,                     #<2> proportion\n    strata = outcome                 #<3> stratify by `gender`\n)\n\ncls_trn <- training(cls_split)       #<4> training set\ncls_tst <- testing(cls_split)        #<5> testing set\n```\n\n## Identify: Recipe {.smaller .scrollable}\n\n- **Recipe**: A blueprint for how to process the data\n\n```{webr-r}\n# Recipe\nbase_rec <-\n  recipe(\n    formula = outcome ~ text,        #<1> outcome ~ predictors\n    data = cls_trn\n  )\n\n# Preview\nbase_rec\n```\n```{webr-r}\n#| context: setup\n# Recipe\nbase_rec <-\n  recipe(\n    formula = outcome ~ text,        #<1> outcome ~ predictors\n    data = cls_trn\n  )\n```\n\n## Identify: Recipe {.smaller .scrollable}\n\n- **Feature selection**: Choosing the most relevant variables\n\n```{webr-r}\n# Feature selection\ntoken_rec <-\n  base_rec |>\n  step_tokenize(text)        #<1> features: tokenized text (by words)\n\n# Preview\ntoken_rec\n```\n\n```{webr-r}\n#| context: setup\n# Feature selection\ntoken_rec <-\n  base_rec |>\n  step_tokenize(text)        #<1> features: tokenized text (by words)\n```\n\n<!-- TF-IDF word tokens (150 tuned) -->\n\n## Identify: Recipe {.smaller .scrollable}\n\n- **Feature engineering**: Deriving new variables and transforming existing ones\n\n```{webr-r}\n# Feature engineering\ntfidf_rec <-\n  token_rec |>\n  step_tokenfilter(text, max_tokens = tune()) |> #<1> limit features\n  step_tfidf(text)                               #<2> feature values (tf-idf)\n\n# Preview\ntfidf_rec\n```\n```{webr-r}\n#| context: setup\n# Feature engineering\ntfidf_rec <-\n  token_rec |>\n  step_tokenfilter(text, max_tokens = tune()) |> #<1> limit features\n  step_tfidf(text)                               #<2> feature values (tf-idf)\n```\n\n## Interrogate: Model selection {.smaller .scrollable}\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n- **Model specification**: A blueprint for the model\n- **Model family**: The type of model to use (e.g., logistic regression, random forest)\n- **Engine**: The software that will fit the model (e.g., `LiblineaR`, `ranger`)\n- **Hyperparameters**: Settings that control the model's behavior (e.g., number of trees in a random forest)\n:::\n\n::: {.column width=\"60%\"}\n| Model | Family | Engine |\n|-------|--------|--------|\n| `logistic_reg()` | Logistic regression | `LiblineaR` |\n| `decision_tree()` | Decision tree | `C5.0` |\n| `random_forest()` | Random forest | `ranger` |\n| `svm_linear()` | Support vector machine | `LiblineaR` |\n\nEach model has hyperparameters that can be tuned to improve performance.\n\n```{webr-r}\n# View hyperparameters\nparsnip::model_db |>\n  filter(mode == \"classification\") |>\n  filter(model == \"decision_tree\") |>\n  unnest(cols = parameters) |>\n  select(model, engine, package, parameter)\n```\n:::\n::::\n\n## Interrogate: Model selection {.smaller .scrollable}\n\n- **Model specification**: A blueprint for the model\n\nThe `logistic_reg()` model has a `penalty` hyperparameter that controls the minimum number of observations in a node. Tuning this parameter and the `max_tokens()` filter will help the model generalize better.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n```{webr-r}\n# Model specification\ncls_spec <-\n  logistic_reg() |>\n  set_engine(\"LiblineaR\")\n\n# Preview\ncls_spec\n```\n:::\n\n::: {.column width=\"50%\"}\n```{webr-r}\n# Model specification\ncls_spec <-\n  logistic_reg(penalty = tune(), mixture = 1) |>\n  set_engine(\"LiblineaR\")\n\n# Preview\ncls_spec\n```\n:::\n::::\n\n```{webr-r}\n#| context: setup\n# Model specification\ncls_spec <-\n  logistic_reg(penalty = tune(), mixture = 1) |>\n  set_engine(\"LiblineaR\")\n```\n\n## Interrogate: Model selection {.smaller .scrollable}\n\nCreate a workflow that combines the recipe and model specification.\n\n```{webr-r}\n# Workflow\ncls_wflow_1 <-\n  workflow() |>\n  add_recipe(tfidf_rec) |>\n  add_model(cls_spec)\n\n# Preview\ncls_wflow_1\n```\n```{webr-r}\n#| context: setup\n\n# Workflow\ncls_wflow_1 <-\n  workflow() |>\n  add_recipe(tfidf_rec) |>\n  add_model(cls_spec)\n```\n\n## Interrogate: Model tuning {.smaller .scrollable}\n\n- **Hyperparameter tuning**: Finding the best settings for the model\n- **Resampling**: Using the training set to estimate how well the model will perform on new (slices of) data\n\n```{webr-r}\n# Grid\np <- parameters(\n  max_tokens(range = c(50, 250)),\n  penalty()\n)\ncls_grid <- grid_regular(p, levels = 5) #<1> hyperparameter range\n\n# Resampling\nset.seed(1234)\ncls_vfold <- vfold_cv(cls_trn, v = 5)  #<2> 5-fold cross-validation\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_1, resamples = cls_vfold, grid = cls_grid)\n```\n```{webr-r}\n#| context: setup\np <- parameters(\n  max_tokens(range = c(50, 250)),\n  penalty()\n)\ncls_grid <- grid_regular(p, levels = 5) #<1> hyperparameter range\n\n# Resampling\nset.seed(1234)\ncls_vfold <- vfold_cv(cls_trn, v = 5)  #<2> 5-fold cross-validation\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_1, resamples = cls_vfold, grid = cls_grid)\n```\n\n## Interrogate: Model tuning {.smaller .scrollable}\n\nChoose the best hyperparameters and finalize the workflow.\n\n```{webr-r}\n# Visualize tuning results\ncls_tune |> autoplot()\n\n# Show/ select best hyperparameters\ncls_tune |> show_best(metric = \"roc_auc\")\ncls_best <- select_best(cls_tune, metric = \"roc_auc\")\n\n# Finalize workflow\ncls_tune_wflow_1 <- finalize_workflow(cls_wflow_1, cls_best)\n```\n```{webr-r}\n#| context: setup\n# Visualize tuning results\ncls_tune |> autoplot()\n\n# Show/ select best hyperparameters\ncls_tune |> show_best(metric = \"roc_auc\")\ncls_best <- select_best(cls_tune, metric = \"roc_auc\")\n\n# Finalize workflow\ncls_tune_wflow_1 <- finalize_workflow(cls_wflow_1, cls_best)\n```\n\n## Interrogate: Fit the model {.smaller .scrollable}\n\n- **Fit the model**: Train the model on the training set\n- **Cross-validation**: Repeatedly train and evaluate the model on different slices of the training set\n\n```{webr-r}\n# Fit the model\ncls_fit_cv_1 <-\n  cls_tune_wflow_1 |>\n  fit_resamples(\n    resamples = cls_vfold,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n```{webr-r}\n#| context: setup\n# Fit the model\ncls_fit_cv_1 <-\n  cls_tune_wflow_1 |>\n  fit_resamples(\n    resamples = cls_vfold,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n\n## Interrogate: Evaluate the model {.smaller .scrollable}\n\n**Performance metrics**: Measures of how well the model is doing\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nClassification\n\n- **Confusion matrix**: A table showing the model's predictions versus the actual outcomes\n- **ROC curve**: A graph showing the trade-off between true positive rate and false positive rate\n:::\n\n::: {.column width=\"50%\"}\nRegression\n\n- **RMSE**: Root mean squared error\n- **Standard deviation of residuals**: How much the model's predictions deviate from the actual outcomes\n:::\n::::\n\n```{webr-r}\n# Collect metrics\ncls_fit_cv_1 |>  collect_metrics()\n\n# Confusion matrix\ncls_fit_cv_1 |> conf_mat_resampled(tidy = FALSE) |> autoplot(type = \"heatmap\")\n```\n\n<!-- Textfeatures ONLY -->\n\n## Identify: Recipe (x2) {.smaller .scrollable}\n\nOur previous feature selection:\n\n- Tokenization: words\n- Feature engineering: tf-idf\n- Feature selection: 150 tokens\n\n```{webr-r}\n# Updated recipe\ncls_rec <-\n  recipe(\n    formula = outcome ~ text,\n    data = cls_trn) |>\n  step_textfeature(text) |>         #<1> default features\n  step_zv(all_predictors()) |>      #<2> remove zero variance\n  step_normalize(all_predictors())  #<3> normalize\n\ncls_rec\n```\n```{webr-r}\n#| context: setup\n# Updated recipe\ncls_rec <-\n  recipe(\n    formula = outcome ~ text,\n    data = cls_trn) |>\n  step_textfeature(text) |>         #<1> default features\n  step_zv(all_predictors()) |>      #<2> remove zero variance\n  step_normalize(all_predictors())  #<3> normalize\n\ncls_rec\n```\n\n## Interrogate: Model selection (x2) {.smaller .scrollable}\n\nUpdate the workflow with the new recipe.\n\n```{webr-r}\n# Update workflow\ncls_wflow_2 <-\n  workflow() |>\n  add_recipe(cls_rec) |>\n  add_model(cls_spec)\n\ncls_wflow_2\n```\n```{webr-r}\n#| context: setup\ncls_wflow_2 <-\n  workflow() |>\n  add_recipe(cls_rec) |>\n  add_model(cls_spec)\n```\n\n## Interrogate: Model tuning (x2) {.smaller .scrollable}\n\nUpdate the grid and resampling.\n\n```{webr-r}\n# Grid\np <- parameters(penalty())\ncls_grid <- grid_regular(p, levels = 10)\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_2, resamples = cls_vfold, grid = cls_grid)\n\ncls_tune |> autoplot()\ncls_tune |> show_best(metric = \"roc_auc\")\n```\n```{webr-r}\n#| context: setup\n# Grid\np <- parameters(penalty())\ncls_grid <- grid_regular(p, levels = 10)\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_2, resamples = cls_vfold, grid = cls_grid)\n```\n\n## Interrogate: Fit the model (x2) {.smaller .scrollable}\n\n- **Fit the model**: Train the model on the training set\n- **Cross-validation**: Repeatedly train and evaluate the model on different slices of the training set\n\n```{webr-r}\n# Select best hyperparameters\ncls_best <- select_best(cls_tune, metric = \"roc_auc\")\n\n# Finalize workflow\ncls_tune_wflow_2 <- finalize_workflow(cls_wflow_2, cls_best)\n\n# Fit the model\ncls_fit_cv_2 <-\n  cls_tune_wflow_2 |>\n  fit_resamples(\n    resamples = cls_vfold,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n```{webr-r}\n#| context: setup\n# Select best hyperparameters\ncls_best <- select_best(cls_tune, metric = \"roc_auc\")\n\n# Finalize workflow\ncls_tune_wflow_2 <- finalize_workflow(cls_wflow_2, cls_best)\n\n# Fit the model\ncls_fit_cv_2 <-\n  cls_tune_wflow_2 |>\n  fit_resamples(\n    resamples = cls_vfold,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n\n## Interrogate: Evaluate the model (x2) {.smaller .scrollable}\n\n**Performance metrics**: Measures of how well the model is doing\n\n```{webr-r}\n# Collect metrics\ncls_fit_cv_2 |> collect_metrics()\n\n# Confusion matrix\ncls_fit_cv_2 |> conf_mat_resampled(tidy = FALSE) |> autoplot(type = \"heatmap\")\n```\n\n<!-- Tokens and textfeatures -->\n\n## Identify: Recipe (x3) {.smaller .scrollable}\n\nOur previous feature selection:\n\n- Tokenization: words\n- Feature engineering: tf-idf\n- Feature selection: 150 tokens\n\n```{webr-r}\n# Updated recipe\ncls_rec <-\n  recipe(\n    formula = outcome ~ text,\n    data = cls_trn) |>\n  step_mutate(text_copy = text) |>\n  step_textfeature(text_copy) |>\n  step_zv(all_predictors()) |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = 150) |>\n  step_tfidf(text) |>\n  step_normalize(all_predictors())\n\ncls_rec |> view_rec()\n```\n```{webr-r}\n#| context: setup\n# Updated recipe\ncls_rec <-\n  recipe(\n    formula = outcome ~ text,\n    data = cls_trn) |>\n  step_mutate(text_copy = text) |>\n  step_textfeature(text_copy) |>\n  step_zv(all_predictors()) |>\n  step_tokenize(text) |>\n  step_tokenfilter(text, max_tokens = 150) |>\n  step_tfidf(text) |>\n  step_normalize(all_predictors())\n\ncls_rec |> view_rec()\n```\n\n## Interrogate: Model selection (x3) {.smaller .scrollable}\n\nUpdate the workflow with the new recipe.\n\n```{webr-r}\ncls_wflow_3 <-\n  workflow() |>\n  add_recipe(cls_rec) |>\n  add_model(cls_spec)\n\ncls_wflow_3\n```\n```{webr-r}\n#| context: setup\ncls_wflow_3 <-\n  workflow() |>\n  add_recipe(cls_rec) |>\n  add_model(cls_spec)\n```\n\n## Interrogate: Model tuning (x3) {.smaller .scrollable}\n\nUpdate the grid and resampling.\n\n```{webr-r}\n# Grid\np <- parameters(penalty())\ncls_grid <- grid_regular(p, levels = 10)\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_3, resamples = cls_vfold, grid = cls_grid)\n\ncls_tune |> autoplot()\ncls_tune |> show_best(metric = \"roc_auc\")\n```\n```{webr-r}\n#| context: setup\n# Grid\np <- parameters(penalty())\ncls_grid <- grid_regular(p, levels = 10)\n\n# Tuning\nset.seed(1234)\ncls_tune <- tune_grid(cls_wflow_3, resamples = cls_vfold, grid = cls_grid)\n```\n\n## Interpret: predict {.smaller}\n\n- **Predictions**: Using the model to make predictions on new data (test set)\n\n```{webr-r}\n# Predict\ncls_final_fit <- last_fit(cls_tune_wflow_2, cls_split)\n```\n\n## Interpret: Evaluate\n\n- **Generalization**: How well the model performs on new data\n\n| **Overfitting** | **Underfitting** |\n|-------------|--------------|\n| When the model performs well on the training set but poorly on new data | When the model performs poorly on both the training set and new data |\n\n```{webr-r}\n# Metrics\ncls_final_fit |> collect_metrics()\n\n# Confusion matrix\ncls_final_fit |> collect_predictions() |> conf_mat(truth = outcome, estimate = .pred_class) |> autoplot(type = \"heatmap\")\n\n# ROC curve\ncls_final_fit |> collect_predictions() |> roc_curve(truth = outcome, .pred_female) |> autoplot()\n```\n\n## Interpret: Evaluate\n\n- **Feature importance**: Which variables are most important for the model's predictions\n\nFor linear models we get coefficients, for tree-based models we get variable importance.\n\n```{webr-r}\n# Feature importance\n\ncls_coefs <-\n  cls_final_fit |>\n  extract_fit_parsnip() |>\n  tidy() |>\n  filter(term != \"(Intercept)\") |>\n  mutate(term = str_remove(term, \"textfeature_text_\"))\n```\n\n## Interpret: Evaluate\n\nWe need to standardize the coefficients to compare them.\n\n```{webr-r}\naes_coefs_z <-\n  cls_coefs |>\n  mutate(z_score = as.vector(scale(estimate)))\n```\n\n```{webr-r}\naes_coefs_z |>\n  ggplot(aes(x = z_score, y = reorder(term, z_score))) +\n  geom_point() +\n  labs(title = \"Feature importance\", x = \"Z-score\", y = \"Feature\")\n```\n\n# Wrap-up\n\n## Final thoughts\n\n- Predictive modeling is a powerful tool for examining relationships in data which can perform tasks (as AI) or provide insights into features that are important for the outcome.\n- The `tidymodels` package provides a consistent and flexible framework for building and evaluating models\n\n## References\n",
    "supporting": [
      "day-20_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}